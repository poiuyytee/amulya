{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poiuyytee/amulya/blob/main/Neural_nets_with_Keras_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTZFFaRxdXNy"
      },
      "source": [
        "## Week 5.1 - Neural_nets with keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxn2ApB2dXOg"
      },
      "source": [
        "# Training Neural Networks with Keras\n",
        "\n",
        "<br />\n",
        "\n",
        "## Tools in the Deep Learning Ecosystem\n",
        "\n",
        "<img src=\"img/deep_learning_ecosystem.jpeg\" />\n",
        "\n",
        "<br />\n",
        "\n",
        "* Hardware GPU vs CPU: At the core hardware level, we have CPUs and GPUs executing instructions.\n",
        "\n",
        "    <br />\n",
        "\n",
        "    * A CPU is able to execute large, sequential instructions but can only execute a small number of instructions parallelly\n",
        "    \n",
        "    <br />\n",
        "    \n",
        "    * A GPU can execute hundreds of small instructions parallelly\n",
        "    \n",
        "<img src = 'img/cpu_vs_gpu.png' />\n",
        "\n",
        "* For deep learning where we have to do a bunch of linear algebraic computations parallelly, GPUs are exponentially faster than CPUs\n",
        "\n",
        "<br />\n",
        "\n",
        "* Frameworks such as BLAS and CUDA help routine computations to be optimised for the specific processor instruction set for accelerated compute\n",
        "\n",
        "    <br />\n",
        "    \n",
        "    * Basic Linear Algebra Subprograms are a bunch of routines that specify the low level routines for many linear algebraic operations. R, numpy, Matlab use BLAS to accelerate linear algebra operations. Ex: Intel's implementation of BLAS is known as Intel Math Kernel Library (MKL)\n",
        "    \n",
        "    <br />\n",
        "    \n",
        "    * CUDA is a framework created by NVIDIA that helps programmers write software to perform general purpose computing tasks on GPUs, most of the deep learning libraries use CUDA\n",
        "\n",
        "<br />\n",
        "\n",
        "* Libraries with support for Autodifferentiation to help in gradient computation for stacked layers were developed such as Theano, Tensorflow, CNTK and PyTorch.\n",
        "\n",
        "* These libraries are operations level (dot product, etc.) and the code is low level and hard to write for quick prototyping of new networks\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/deep_learning_frameworks.png' width='550px'/>\n",
        "\n",
        "<br />\n",
        "\n",
        "* That is where Keras comes into picture, it is a high level library with the abstraction at the __layer__ level. It was built as an abstraction to Theano and later support was added to Tensorflow and CNTK\n",
        "\n",
        "* So you can write code in Keras that can run on any of these three deep learning library __backends__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcIK-miIdXO2"
      },
      "source": [
        "## The Building Blocks of Neural Networks\n",
        "\n",
        "### Layers, Data and Learning Representations\n",
        "\n",
        "<br />\n",
        "\n",
        "* __Layers__: logically grouped operations in a neural network, the parameters for the operations in the layer learn to generate the best features to predict the target\n",
        "\n",
        "<img src='img/nn_layers.jpeg' width='400px'/>\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "* The network needs to have __input data__ and corresponding __targets (y)__\n",
        "\n",
        "<br />\n",
        "\n",
        "* In traditional machine learning we see that changing the representation of the data (kernel trick, etc.) helps ease the process of learning from data\n",
        "\n",
        "<br />\n",
        "\n",
        "* __Activation function__ adds that non linearity and in combination with weights (parameters) of a layer, the network learns better representations of the data at each layer\n",
        "\n",
        "<br />\n",
        "\n",
        "### So basically, each layer takes input as data and spits out transformed data as output, simple as that. Now, let's dive into the details\n",
        "\n",
        "<img src='img/learning_representations_mlp.jpg' />\n",
        "\n",
        "<br />\n",
        "\n",
        "* The goal of training neural networks is to find these perfect representation of data, which we get by \"learning\" the right weights\n",
        "\n",
        "<img src='img/learning_weights.jpg' />\n",
        "\n",
        "<br />\n",
        "\n",
        "* The loss function, which defines the feedback signal used for learning helps guage __how different are the targets and the predicted targets__\n",
        "\n",
        "<img src='img/loss_function.jpg' />\n",
        "\n",
        "<br />\n",
        "\n",
        "* The optimizer, based on the feedback signal from the loss function changes the parameters / weights of the network to help make the predictions as close to the target as possible (minimizing the loss function)\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/building_blocks_of_neural_networks.jpg' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0BeJ4HQdXPA"
      },
      "source": [
        "## The Keras Interface\n",
        "\n",
        "<br />\n",
        "\n",
        "* There are two major ways to define and run neural netwroks using the Keras API\n",
        "    \n",
        "    <br />\n",
        "    \n",
        "    * Sequential API\n",
        "    \n",
        "    <br />\n",
        "    \n",
        "    * Functional API\n",
        "\n",
        "<br />\n",
        "\n",
        "### Keras API\n",
        "\n",
        "<img src=\"img/keras_interface.jpg\" width='550px'/>\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/keras_sequential_api.jpg' />\n",
        "\n",
        "<br />\n",
        "\n",
        "### The Functional API : Chainaing the layers/ callable layers\n",
        "\n",
        "* The functional api allows us to build complex graph networks, we can kee chaining the the layers as functions and finally the `Model(inputs, outputs)` class connects all the various inputs and outputs\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src=\"img/functional_api_bimodal_network.png\" width='450px'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "937IgWbAdXPF"
      },
      "source": [
        "Keras Core layers\n",
        "\n",
        "#https://keras.io/api/layers/core_layers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdH1UvATdXPK"
      },
      "source": [
        "## Layers in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgt4clMxdXPT"
      },
      "source": [
        "* There are different categories of layers in Keras, the most commonly used ones are \"Dense Layers\", \"Convolution Layers\", \"Recurrent Layers\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBw7Hx8JdXPe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLgLW_WMdXPs"
      },
      "source": [
        "* Let's break down the above layer that we just built using the Dense class from keras\n",
        "\n",
        "<br />\n",
        "\n",
        "* The OUTPUT from the above layer can be given by `sigmoid ( dot ( W, INPUT ) + b )`, in the first round of training the W (weights) are randomly \"initialized\"\n",
        "\n",
        "<br />\n",
        "\n",
        "* Every input to the Dense layer (also known as fully connected) is connected to every unit in the hidden layer, as shown in the figure below\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src ='img/fc_dense_layers_keras.jpg' />\n",
        "\n",
        "<br />\n",
        "\n",
        "* We will see many more categories and types of __Layers__ in keras, the __\"Dense\"__ layer is just one such class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Lz4GITdXPy"
      },
      "source": [
        "##  method 1: The Keras Sequential API : Stacking the layers together\n",
        "\n",
        "<br />\n",
        "\n",
        "* The sequential api allows us to __quickly stack layers__ and build networks\n",
        "\n",
        "\n",
        "* The keras sequential api enables us to build common yet complex neural network architectures flexibly\n",
        "\n",
        "<br />\n",
        "\n",
        "* Objects of the Keras sequential class, can have multiple neural network layers stacked on top of one another\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/keras_sequential_api.jpg' />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7OdBO2NdXP9"
      },
      "source": [
        "\n",
        "* You can create a Keras sequential model by passing in a list of layers to the Sequential object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrgtwK4cdXP_"
      },
      "outputs": [],
      "source": [
        "#example 1\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model1 = Sequential([\n",
        "    Dense(2, input_shape=(2,), activation = 'sigmoid'),\n",
        "    Dense(1, activation = 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_npMVD37dXQC",
        "outputId": "eb587c3f-b626-4164-b186-a98bc3bf97a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9TD-I23dXQP"
      },
      "source": [
        "<img src='img/Dense_prams1.jpeg' width='550px' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeldzuXWdXQR"
      },
      "source": [
        "* But soon this becomes a problem to add more layers in a single list, so the \"add\" method on the sequential class object can add layers sequentially to the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0xcrrJYdXQT"
      },
      "outputs": [],
      "source": [
        "#example 2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model2 = Sequential([\n",
        "    Dense(3, input_shape=(3,), activation = 'sigmoid'),\n",
        "    Dense(3, activation = 'sigmoid'),\n",
        "    Dense(2, activation = 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WOXmR-odXQU",
        "outputId": "72600438-91cc-40a8-af40-23e0db98e55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 3)                 12        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 12        \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32\n",
            "Trainable params: 32\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seY1MCNidXQb"
      },
      "source": [
        "<img src='img/Dense_prams2.jpeg' width='550px' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr-78X71dXQc"
      },
      "source": [
        "## using add() method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC128EEidXQc"
      },
      "outputs": [],
      "source": [
        "#example 1\n",
        "neural_network = Sequential()\n",
        "\n",
        "neural_network.add(Dense(2, input_dim=2, activation = 'sigmoid',\n",
        "                         kernel_initializer = 'zeros', bias_initializer = 'zeros'))\n",
        "\n",
        "\n",
        "neural_network.add(Dense(1, activation = 'sigmoid', kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'zeros'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B09xS-QqdXQd",
        "outputId": "31b6b650-bfae-4835-d581-7565fd6ec961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "neural_network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18XxSxiddXQg"
      },
      "outputs": [],
      "source": [
        "#example 2\n",
        "neural_network = Sequential()\n",
        "\n",
        "neural_network.add(Dense(3, input_dim=3, activation = 'sigmoid',name = 'layer_1',\n",
        "                         kernel_initializer = 'zeros', bias_initializer = 'zeros'))\n",
        "\n",
        "neural_network.add(Dense(3, activation = 'sigmoid',name = 'layer_2', kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'zeros'))\n",
        "\n",
        "neural_network.add(Dense(2, activation = 'sigmoid', name = 'layer_3',kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'zeros'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfuIVQb3dXQj",
        "outputId": "eb64ed71-46c0-483f-f1bf-c90b7c1e27c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer_1 (Dense)             (None, 3)                 12        \n",
            "                                                                 \n",
            " layer_2 (Dense)             (None, 3)                 12        \n",
            "                                                                 \n",
            " layer_3 (Dense)             (None, 2)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32\n",
            "Trainable params: 32\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "neural_network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkuWbK3VdXQm"
      },
      "outputs": [],
      "source": [
        "#example 3\n",
        "nn = Sequential()\n",
        "\n",
        "nn.add(Dense(1, input_dim=1, activation = 'sigmoid',name = 'layer_1',\n",
        "                         kernel_initializer = 'zeros', bias_initializer = 'zeros'))\n",
        "\n",
        "nn.add(Dense(1, activation = 'sigmoid', name = 'layer_2',kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'zeros'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ80w1hidXRm",
        "outputId": "99dd8ac9-35e4-4a30-d9a1-ee7ed101f939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer_1 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            " layer_2 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4\n",
            "Trainable params: 4\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "nn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGMoqju4dXRo",
        "outputId": "f573f93f-81b0-44e5-968f-ae4cb80549a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights and biases of the layers before training the model: \n",
            "\n",
            "layer_1\n",
            "Weights\n",
            "Shape:  (1, 1) \n",
            " [[0.]]\n",
            "Bias\n",
            "Shape:  (1,) \n",
            " [0.] \n",
            "\n",
            "layer_2\n",
            "Weights\n",
            "Shape:  (1, 1) \n",
            " [[0.]]\n",
            "Bias\n",
            "Shape:  (1,) \n",
            " [0.] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Weights and biases of the layers before training the model: \\n\")\n",
        "for layer in nn.layers:\n",
        "  print(layer.name)\n",
        "  print(\"Weights\")\n",
        "  print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
        "  print(\"Bias\")\n",
        "  print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0F9iS-RdXRq"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFWTDkRKdXRs",
        "outputId": "68b4c810-cfdc-4792-cf5c-641513aa4570"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[0.1744399]]),\n",
              " array([0.24087493]),\n",
              " array([[0.56080447]]),\n",
              " array([0.88717866])]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights = [np.random.rand(*w.shape) for w in nn.get_weights()]\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7sIZ4MpdXRt"
      },
      "outputs": [],
      "source": [
        "nn.set_weights(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Drdk-BddXRu"
      },
      "outputs": [],
      "source": [
        "weights=[[[1]],[[1]],[[1]],[[1]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71mDY5pAdXRv",
        "outputId": "97a6eb16-4cff-43a7-863b-b37fc84038ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weights:  [array([[0.1744399]], dtype=float32), array([0.24087493], dtype=float32), array([[0.5608045]], dtype=float32), array([0.88717866], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "print(\"weights: \",nn.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYO9axnFdXRw",
        "outputId": "fe05a50b-9e5f-444c-b6d3-a7b1df01135e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(nn.non_trainable_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geH_fxsjdXRy",
        "outputId": "b831666f-d973-4d6d-dc20-fa6e54b26896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'dense_23/kernel:0' shape=(3, 3) dtype=float32, numpy=\n",
            "array([[0., 0., 0.],\n",
            "       [0., 0., 0.],\n",
            "       [0., 0., 0.]], dtype=float32)>, <tf.Variable 'dense_23/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_24/kernel:0' shape=(3, 3) dtype=float32, numpy=\n",
            "array([[0., 0., 0.],\n",
            "       [0., 0., 0.],\n",
            "       [0., 0., 0.]], dtype=float32)>, <tf.Variable 'dense_24/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_25/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
            "array([[0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.]], dtype=float32)>, <tf.Variable 'dense_25/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"
          ]
        }
      ],
      "source": [
        "print(nn.trainable_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASDYtWMjdXR0"
      },
      "outputs": [],
      "source": [
        "#example 4\n",
        "nn = Sequential()\n",
        "\n",
        "nn.add(Dense(1, input_dim=1, activation = 'sigmoid',name = 'layer_1',\n",
        "                         kernel_initializer = 'zeros', bias_initializer = 'zeros'))\n",
        "\n",
        "nn.add(Dense(1, activation = 'sigmoid', name = 'layer_2',kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'zeros'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjv_M5X_dXR1",
        "outputId": "14b188d5-09b1-4b80-f9ce-9ba5bf925bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer_1 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            " layer_2 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4\n",
            "Trainable params: 4\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "nn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGK0ty9CdXR2",
        "outputId": "2cb63dfa-50ad-495a-8353-962522ae29eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tf.Variable 'layer_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.1744399]], dtype=float32)>,\n",
              " <tf.Variable 'layer_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.24087493], dtype=float32)>,\n",
              " <tf.Variable 'layer_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.5608045]], dtype=float32)>,\n",
              " <tf.Variable 'layer_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.88717866], dtype=float32)>]"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.trainable_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ6ZjoRydXR3"
      },
      "outputs": [],
      "source": [
        "               ##############################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFs1RxAYdXR4"
      },
      "source": [
        "## Method 2: Building the same network using the Keras Functional API\n",
        "\n",
        "<br />\n",
        "\n",
        "* Essential keywords of the functional API: `Tensor, Callable Layers, Input, Model`\n",
        "\n",
        "<br />\n",
        "\n",
        "* The core data structure for the functional API is the `tensor`\n",
        "\n",
        "<br />\n",
        "\n",
        "* Once you have a tensor the others are just layers similar to the sequential API. But in the case of the functional API, the Layers become `callable` using round paranthesis just like we call a function, for example `function_name()`\n",
        "\n",
        "* The output of calling any `Layer` object in `Keras` is a `tensor`.\n",
        "\n",
        "We pass in a tensor to a layer, as follows\n",
        "\n",
        "```\n",
        "                                     output_tensor = Layer(layer_params)(tensor)\n",
        "\n",
        "```\n",
        "\n",
        "<br />\n",
        "\n",
        "* To pass our initial tensor, or the input data to the first layer, we have to create an input tensor, which we can by importing the `Input` class from the `keras.layers` submodule, as follows\n",
        "\n",
        "```\n",
        "                                     from keras.layers import Input, Dense\n",
        "                                        \n",
        "                                     input_tensor = Input(shape = (num_independent_vars, ))\n",
        "                                        \n",
        "                                     first_layer_output = Dense(layer_params)(input_tensor)\n",
        "                                        \n",
        "                                     output = Dense(final_layer_params)(first_layer_output)\n",
        "\n",
        "```\n",
        "\n",
        "* Finally, we need to import the `Model` class from the `keras.models` submodule and mention the inputs and the outputs of the model, after which `Keras` very gracefully builds a model connecting the inputs to the outputs\n",
        "\n",
        "```\n",
        "                                     from keras.models import Model\n",
        "\n",
        "                                     model = Model(inputs = input_tensor, outputs = output)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDVB6ElkdXR8"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# define input\n",
        "input_tensor = Input(shape = (2,))\n",
        "\n",
        "# define layers\n",
        "output1 = Dense(2, activation = 'sigmoid')(input_tensor)\n",
        "output2 = Dense(1, activation = 'sigmoid')(output1)\n",
        "\n",
        "model_functional = Model(inputs = input_tensor, outputs = output2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv0fTELBdXSb",
        "outputId": "a5abdcf2-71f8-4396-bda4-7ee95cb573e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_functional.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpcBNhIJdXSd"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "input_tensor = Input(shape = (3,))\n",
        "\n",
        "output1 = Dense(3, activation = 'sigmoid')(input_tensor)\n",
        "output2 = Dense(3, activation = 'sigmoid')(output1)\n",
        "output3 = Dense(2, activation = 'sigmoid')(output2)\n",
        "\n",
        "model_functional = Model(inputs = input_tensor, outputs = output3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRJ0y4MsdXSe",
        "outputId": "30894bc4-6f63-470a-9777-8ee8a11d98da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 3)]               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 3)                 12        \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 3)                 12        \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 2)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32\n",
            "Trainable params: 32\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_functional.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ggxYtUTdXSg"
      },
      "source": [
        "* The model summary looks the same, except for the inclusion of the Input Layer which creates the input tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89--DPRXdXSh"
      },
      "source": [
        "## Defining, Compiling and Running a Neural Network in Keras\n",
        "\n",
        "<br />\n",
        "\n",
        "### The process of learning in a Neural Network\n",
        "\n",
        "<br />\n",
        "\n",
        "### Loss Score\n",
        "\n",
        "<br />\n",
        "\n",
        "* The loss score is feedback signal that says how far is the output of your network compared to the ground truth\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/loss_function.jpg' />\n",
        "\n",
        "<br />\n",
        "\n",
        "* Two such loss scores that we use quite frequently are :\n",
        "    \n",
        "    1) Binary Cross-Entropy: For Two-Class Classification problems\n",
        "    \n",
        "    2) Mean Squared Error: For Regression problems\n",
        "    \n",
        "<br />\n",
        "\n",
        "* We have already come across mean squared error before, so let's dive deeper into Binary Cross Entropy\n",
        "\n",
        "<br />\n",
        "\n",
        "$$\\begin{eqnarray}\n",
        "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln p + (1-y ) \\ln (1-p) \\right]\n",
        "\\end{eqnarray}$$\n",
        "\n",
        "<br />\n",
        "\n",
        "* In the above equation, p is the output of the network, n is the total number of samples in the training data, the sum is over all training inputs, x, and y is the corresponding desired output\n",
        "\n",
        "<br />\n",
        "\n",
        "* Below, we see the value of the cross entropy (sometimes referred to as the log loss) changing with the predicted probability, we can see that the value of the loss for a prediction above 0.5 significantly drops and this helps the network converge much faster than using a traditional mean squared error\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/binary_cross_entropy.png' width='300px'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEwMQdQkdXSj"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "<br />\n",
        "\n",
        "* An optimizer is an algorithm that uses the feedback signal from the loss function, to actually update the weights so that the output from the network gets closer to the ground truth. The first optimizer that we use is Stochastic Gradient Descent (SGD), we will slowly come across many more optimizers\n",
        "\n",
        "<br />\n",
        "\n",
        "* We can import Classes from the optimizers module of keras and customize the specific optimizer to our liking\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/building_blocks_of_neural_networks.jpg' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvgHT69rdXSk"
      },
      "source": [
        "* Please refer to this beautiful article written by Sebastian Ruder on the various iterations of the SGD optimizer http://ruder.io/optimizing-gradient-descent/\n",
        "\n",
        "<br />\n",
        "\n",
        "* Keras has the implementations of the following optimizers:  \n",
        "    * SGD\n",
        "    * RMSprop\n",
        "    * Adagrad\n",
        "    * Adadelta\n",
        "    * Adam\n",
        "    * Adamax\n",
        "    * Nadam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm7yH5HUdXSm"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "customized_optimizer = SGD(lr = 0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8iSB1didXSn"
      },
      "source": [
        "* We already know how learning rate can effect convergence, the graph below provides a decent intuition, hence having the flexibility to change the learning rate is very important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v3hmDvqdXSo"
      },
      "source": [
        "![](img/learning_rate.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyy8KUDRdXSp"
      },
      "source": [
        "## Compiling the neural network ( loss function + optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la9JM184dXSr"
      },
      "outputs": [],
      "source": [
        "neural_network = Sequential()\n",
        "\n",
        "neural_network.add(Dense(32, input_dim=784, activation = 'sigmoid',\n",
        "                         kernel_initializer = 'RandomUniform', bias_initializer = 'zeros'))\n",
        "\n",
        "neural_network.add(Dense(10, activation = 'sigmoid', kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'RandomUniform'))\n",
        "\n",
        "neural_network.add(Dense(1, activation = 'sigmoid', kernel_initializer = 'zeros',\n",
        "                         bias_initializer = 'RandomUniform'))\n",
        "\n",
        "neural_network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESPWgOjSdXSu"
      },
      "outputs": [],
      "source": [
        "# neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgBg5d6ddXSv"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "customized_optimizer = SGD(lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRpxFlE1dXSx"
      },
      "outputs": [],
      "source": [
        "neural_network.compile(loss = 'binary_crossentropy', optimizer = customized_optimizer, metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "460mmoEVdXSy"
      },
      "source": [
        "* As we can see from the compile step above we need to specify the loss function, optimization algorithm and we can also mention the metrics that we want to monitor while training the neural network\n",
        "\n",
        "<br />\n",
        "\n",
        "* Also, please __note__ that the optimizer argument can either be a string or a customizable object from the optimizers module in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gue1iezGdXSz"
      },
      "source": [
        "## So are we done? what about learning the weights?\n",
        "\n",
        "<br />\n",
        "\n",
        "* Training the network in Keras is also very simple, we call the `.fit()` method and pass in the arguments\n",
        "\n",
        "<br />\n",
        "\n",
        "* Some important terms for training neural networks are epochs, batch_size"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "oNejqY1GdXS0"
      },
      "source": [
        "# NOTE : Don't run the following line of code as we do not yet have X_train and y_train\n",
        "\n",
        "neural_network.fit(X_train, y_train, epochs=100, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfqf7etdXS2"
      },
      "source": [
        "### Download the MNIST dataset using the Keras helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-CA_0dGdXS3"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD-HFVJqdXS7"
      },
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl28fzRSdXS8"
      },
      "source": [
        "### The dataset consist of images represented as\n",
        "\n",
        "![](img/mnist_representation.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj1ck8thdXS_"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUlfe82AdXTB"
      },
      "outputs": [],
      "source": [
        "## Reshape the images from 28 * 28 to 784\n",
        "\n",
        "X_train = X_train.reshape(-1, 28*28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2-naD0tdXTC"
      },
      "outputs": [],
      "source": [
        "X_train[2349]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiCh8igAdXTD"
      },
      "outputs": [],
      "source": [
        "y_train[2349]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI2HAjpZdXTF"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuAyAjF0dXTG"
      },
      "outputs": [],
      "source": [
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkcnKYC7dXTG"
      },
      "outputs": [],
      "source": [
        "## Reshape the images from 28 * 28 to 284\n",
        "\n",
        "X_test = X_test.reshape(-1, 28*28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1JgpIuPdXTJ"
      },
      "outputs": [],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZGnUZOj3dXTK"
      },
      "outputs": [],
      "source": [
        "training_history = neural_network.fit(X_train, y_train, epochs=100, batch_size=1024, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH6xMYDFdXTM"
      },
      "source": [
        "## Plotting the training process of the neural network\n",
        "\n",
        "<br />\n",
        "\n",
        "* Once, we write the code for plotting the loss and accuracy while training our network, we will functionalize the code to make it easy for us to very quickly visualize the training metrics of our network.\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of_c9ZcPdXTd"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(training_history.history['accuracy'])\n",
        "plt.plot(training_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgCXdkHcdXTf"
      },
      "outputs": [],
      "source": [
        "print(training_history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvWVWBs2dXTi"
      },
      "outputs": [],
      "source": [
        "plt.plot(training_history.history['loss'])\n",
        "plt.plot(training_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvp9HvwOdXTl"
      },
      "outputs": [],
      "source": [
        "# Function to plot metrics after training\n",
        "\n",
        "def plot_training_metrics(history_object, metric = 'loss', val = True):\n",
        "\n",
        "    plt.plot(history_object.history[metric])\n",
        "\n",
        "    if val == True:\n",
        "        plt.plot(history_object.history['val_' + metric])\n",
        "\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uP5XS4gzdXTt"
      },
      "outputs": [],
      "source": [
        "plot_training_metrics(training_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLvnZn21dXTx"
      },
      "outputs": [],
      "source": [
        "plot_training_metrics(training_history, metric = 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Tb24eBdXTz"
      },
      "source": [
        "* An __Epoch__ is when an ENTIRE dataset is passed forward and backward through the neural network only once.\n",
        "\n",
        "<br />\n",
        "\n",
        "* Since most of the times an epoch is too large to fit in memory, we divide the data into batches and compute the gradient on batches for each forward and backward pass\n",
        "\n",
        "<br />\n",
        "\n",
        "* __Batch size__ is the number of samples that are going to be propagated through the network.\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/learning_rate_epochs_rel.png' width='400px'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeraP4bedXUH"
      },
      "source": [
        "## Neural Network Architectures for Basic ML Tasks\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src='img/nn_for_basic_ml_tasks.jpg' width='800px'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TKFu8iXdXUJ"
      },
      "source": [
        "## Exercise & Experiment\n",
        "\n",
        "<br />\n",
        "\n",
        "* Use different activation functions, report difference in metrics while training\n",
        "\n",
        "__Different experiments to run: 'sigmoid', 'tanh', 'relu'__\n",
        "\n",
        "<br />\n",
        "\n",
        "* Use different weight initialization functions, report difference in metrics while training\n",
        "\n",
        "__Different experiments to run: 'zeros', 'RandomUniform', 'RandomNormal', 'glorot_normal', 'glorot_uniform', 'he_normal', 'lecun_normal', 'he_uniform'__\n",
        "\n",
        "<br />\n",
        "\n",
        "* Use different number of layers, number of nodes and report difference in metrics while training\n",
        "\n",
        "__Different experiments to run: more nodes or more layers?__\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "* Use BatchNormalization, report difference in metrics while training, if you have to add batchnorm after a layer just do as follows\n",
        "\n",
        "```\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "model.add(Dense(layer_params))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(layer_params))\n",
        "\n",
        "```\n",
        "\n",
        "<br />\n",
        "\n",
        "* Use different Dropout rates, report difference in metrics while training\n",
        "\n",
        "```\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model.add(Dense(layer_params))\n",
        "\n",
        "model.add(Dropout(rate = 0.4))\n",
        "\n",
        "model.add(Dense(layer_params))\n",
        "\n",
        "```\n",
        "\n",
        "<br />\n",
        "\n",
        "* Use different learning rates and optimizers, report difference in metrics while training\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIaNcSNddXUL"
      },
      "source": [
        "# There are no secrets to success. It is the result of preparation, hard work, and learning from failure."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "wPnju3sVdXUM"
      },
      "source": [
        "######################################################################################################################"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}